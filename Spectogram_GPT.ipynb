{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnida/spectogram-gpt-features/blob/main/Spectogram_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xxStzi9B6WaD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from typing import List, Optional, Union\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
        "from sae_analysis.visualizer import data_fns, html_fns\n",
        "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\" \n",
        "else:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def imshow(x, **kwargs):\n",
        "    x_numpy = utils.to_numpy(x)\n",
        "    px.imshow(x_numpy, **kwargs).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "print(torch.device(device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSVHP6lffzNB"
      },
      "source": [
        "Our goal here is to create a mel spectrogram type of visualization of GPT-2 Sparse Autoencoder (SAE) different features. The goal is to visualization how features are alike or different.\n",
        "\n",
        "Inspired entirely by @thesephist's tweet: https://x.com/thesephist/status/1754322902210793727?s=20\n",
        "\n",
        "Using the SAE from Joseph Isaac Bloom:\n",
        "https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load in the Sparse Autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n",
            "Moving model to device:  mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mnida/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is not tokenized! Updating config.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "path = \"artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_6144:v2/1200001024_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_6144.pt\"\n",
        "model, sparse_autoencoder, activations_loader = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
        "    path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1900 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "with open('data.txt', 'r') as file:\n",
        "    text_data = file.read()\n",
        "tokenized_text = model.tokenizer.encode(text_data)\n",
        "tokenized_text_tensor = torch.tensor(tokenized_text).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Truncate or pad the tokenized text to fit the model's input size\n",
        "max_input_size = 1024  # Replace this with your model's max input size\n",
        "if len(tokenized_text_tensor[0]) > max_input_size:\n",
        "    tokenized_text_tensor = tokenized_text_tensor[:, :max_input_size]\n",
        "elif len(tokenized_text_tensor[0]) < max_input_size:\n",
        "    padding = torch.zeros((1, max_input_size - len(tokenized_text_tensor[0])))\n",
        "    tokenized_text_tensor = torch.cat((tokenized_text_tensor, padding), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024])\n",
            "HookedTransformer(\n",
            "  (embed): Embed()\n",
            "  (hook_embed): HookPoint()\n",
            "  (pos_embed): PosEmbed()\n",
            "  (hook_pos_embed): HookPoint()\n",
            "  (blocks): ModuleList(\n",
            "    (0-11): 12 x TransformerBlock(\n",
            "      (ln1): LayerNormPre(\n",
            "        (hook_scale): HookPoint()\n",
            "        (hook_normalized): HookPoint()\n",
            "      )\n",
            "      (ln2): LayerNormPre(\n",
            "        (hook_scale): HookPoint()\n",
            "        (hook_normalized): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_pattern): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "      )\n",
            "      (hook_attn_in): HookPoint()\n",
            "      (hook_q_input): HookPoint()\n",
            "      (hook_k_input): HookPoint()\n",
            "      (hook_v_input): HookPoint()\n",
            "      (hook_mlp_in): HookPoint()\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "  )\n",
            "  (ln_final): LayerNormPre(\n",
            "    (hook_scale): HookPoint()\n",
            "    (hook_normalized): HookPoint()\n",
            "  )\n",
            "  (unembed): Unembed()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_text_tensor.shape)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sum'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m _, cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(batch_tokens, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cache[sparse_autoencoder\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_point] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m sae_out, feature_acts, loss, mse_loss, l1_loss \u001b[38;5;241m=\u001b[39m \u001b[43msparse_autoencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_point\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m cache\n",
            "File \u001b[0;32m~/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/spectogram-gpt-features/sae_training/sparse_autoencoder.py:105\u001b[0m, in \u001b[0;36mSparseAutoencoder.forward\u001b[0;34m(self, x, dead_neuron_mask)\u001b[0m\n\u001b[1;32m    103\u001b[0m mse_loss_ghost_resid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# gate on config and training so evals is not slowed down.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_ghost_grads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mdead_neuron_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m dead_neuron_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# ghost protocol\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# 1.\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sum'"
          ]
        }
      ],
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    batch_tokens = activations_loader.get_batch_tokens()\n",
        "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
        "    print(cache[sparse_autoencoder.cfg.hook_point] is None)\n",
        "    sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(\n",
        "        cache[sparse_autoencoder.cfg.hook_point]\n",
        "    )\n",
        "    del cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xvTbaI7k6fOR",
        "outputId": "904671b9-90c3-4be0-c11c-a78047ae57dc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m      3\u001b[0m feature_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Your feature vectors from the autoencoder\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=1, perplexity=30, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(feature_acts.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skdfvQhNoDO3"
      },
      "source": [
        "## Visualize the Spectogram with Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NniR8ydroHtc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "times = np.arange(len(tsne_results))  # Assuming each feature vector corresponds to a time token\n",
        "frequencies = tsne_results.flatten()  # Your 1D t-SNE mapped to frequencies\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.scatter(times, frequencies, c=frequencies, cmap='viridis')  # Color by frequency for visualization\n",
        "plt.colorbar(label='Frequency')\n",
        "plt.xlabel('Time (tokens)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.title('Spectrogram Representation')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOb2svWdUXcihmI08beYbBX",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
